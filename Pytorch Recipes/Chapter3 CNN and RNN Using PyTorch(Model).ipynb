{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de42ae58",
   "metadata": {},
   "source": [
    "# CHAPTER 3\n",
    "**CNN and RNN Using PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b86d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492db32",
   "metadata": {},
   "source": [
    "## Recipe 3-1. Setting Up a Loss Function\n",
    "In this recipe, we use another tensor as the update variable, and introduce the tensors to the sample model and compute the error or loss. Then we compute the rate of change in the loss function to measure the choice of loss function in model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5862e3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = tensor([1.7813])\n",
      "b = tensor([33.0874])\n",
      "x * w + b = tensor([33.9781, 58.0259, 59.8072, 82.9644, 52.6819, 47.3380, 38.4314, 25.9621,\n",
      "        43.7753, 56.2446, 70.4952])\n",
      "loss = tensor(9.7847)\n"
     ]
    }
   ],
   "source": [
    "dw = 0.01\n",
    "db = 0.01\n",
    "learning_rate = 0.001\n",
    "epoches = 10000\n",
    "\n",
    "def forward(x, w, b):\n",
    "    return w * x + b\n",
    "\n",
    "def train(x, y, w, b, loss_fn):\n",
    "    for epoch in range(epoches):\n",
    "        # gradient\n",
    "        grad_w = (loss_fn(forward(x, w + dw, b), y) - loss_fn(forward(x, w, b), y)) / dw\n",
    "        grad_b = (loss_fn(forward(x, w, b + db), y) - loss_fn(forward(x, w, b), y)) / db\n",
    "        # back propagation\n",
    "        w -= learning_rate * grad_w\n",
    "        b -= learning_rate * grad_b\n",
    "    return w, b\n",
    "\n",
    "x = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n",
    "y = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "w = torch.ones(1)\n",
    "b = torch.zeros(1)\n",
    "w, b = train(x, y, w, b, functional.mse_loss)\n",
    "print('w =', w)\n",
    "print('b =', b)\n",
    "print('x * w + b =', x * w + b)\n",
    "print('loss =', functional.mse_loss(x * w + b, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa23ac4",
   "metadata": {},
   "source": [
    "## Recipe 3-2. Estimating the Derivative of the Loss Function\n",
    "How do we estimate the derivative of a loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84123499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = tensor([3.4466])\n",
      "b = tensor([6.4415])\n",
      "x * w + b = tensor([  8.1648,  54.6933,  58.1398, 102.9451,  44.3536,  34.0139,  16.7811,\n",
      "         -7.3448,  27.1208,  51.2467,  78.8192])\n",
      "loss = tensor(14.8905)\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones(1)\n",
    "b = torch.zeros(1)\n",
    "w, b = train(x, y, w, b, functional.l1_loss)\n",
    "print('w =', w)\n",
    "print('b =', b)\n",
    "print('x * w + b =', x * w + b)\n",
    "print('loss =', functional.l1_loss(x * w + b, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f072d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = tensor([3.5220])\n",
      "b = tensor([5.4845])\n",
      "x * w + b = tensor([  7.2455,  54.7926,  58.3146, 104.1006,  44.2266,  33.6606,  16.0505,\n",
      "         -8.6035,  26.6165,  51.2706,  79.4466])\n",
      "loss = tensor(14.9359)\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones(1)\n",
    "b = torch.zeros(1)\n",
    "w, b = train(x, y, w, b, functional.huber_loss)\n",
    "print('w =', w)\n",
    "print('b =', b)\n",
    "print('x * w + b =', x * w + b)\n",
    "print('loss =', functional.huber_loss(x * w + b, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bff1e05",
   "metadata": {},
   "source": [
    "## Recipe 3-3. Fine-Tuning a Model\n",
    "Using the backward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee3a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_b.grad = tensor([-989.5273,  -82.6000])\n"
     ]
    }
   ],
   "source": [
    "w_b = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "loss = functional.mse_loss(w_b[0] * x + w_b[1], y)\n",
    "# calculate gradients\n",
    "loss.backward()\n",
    "print('w_b.grad =', w_b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3395b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_b.grad = tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# reset the grad, otherwise it will accumulate\n",
    "if w_b.grad is not None:\n",
    "    w_b.grad.zero_()\n",
    "print('w_b.grad =', w_b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9ff8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = tensor(1.7928, grad_fn=<SelectBackward>)\n",
      "b = tensor(32.9719, grad_fn=<SelectBackward>)\n",
      "x * w + b = tensor([33.8683, 58.0710, 59.8638, 83.1701, 52.6927, 47.3143, 38.3503, 25.8008,\n",
      "        43.7287, 56.2782, 70.6206], grad_fn=<AddBackward0>)\n",
      "loss = tensor(9.7754, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "w_b = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "# nearly torch version\n",
    "for epoch in range(epoches):\n",
    "    loss = functional.mse_loss(w_b[0] * x + w_b[1], y)\n",
    "    if w_b.grad is not None:\n",
    "        w_b.grad.zero_()\n",
    "    loss.backward()\n",
    "    w_b = (w_b - learning_rate * w_b.grad).detach().requires_grad_()\n",
    "\n",
    "print('w =', w_b[0])\n",
    "print('b =', w_b[1])\n",
    "print('x * w + b =', x * w_b[0] + w_b[1])\n",
    "print('loss =', functional.mse_loss(x * w_b[0] + w_b[1], y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089b20d",
   "metadata": {},
   "source": [
    "## Recipe 3-4. Selecting an Optimization Function\n",
    "Optimize the gradients with the function in Recipe 3-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70051b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimization class\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a096e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = tensor(1.7928, grad_fn=<SelectBackward>)\n",
      "b = tensor(32.9719, grad_fn=<SelectBackward>)\n",
      "x * w + b = tensor([33.8683, 58.0710, 59.8638, 83.1701, 52.6927, 47.3143, 38.3503, 25.8008,\n",
      "        43.7287, 56.2782, 70.6206], grad_fn=<AddBackward0>)\n",
      "loss = tensor(9.7754, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "w_b = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "# SGD version\n",
    "optimizer = optim.SGD([w_b], lr=learning_rate)\n",
    "# quite-nearly torch version\n",
    "for epoch in range(epoches):\n",
    "    loss = functional.mse_loss(w_b[0] * x + w_b[1], y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('w =', w_b[0])\n",
    "print('b =', w_b[1])\n",
    "print('x * w + b =', x * w_b[0] + w_b[1])\n",
    "print('loss =', functional.mse_loss(x * w_b[0] + w_b[1], y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7de38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = tensor(1.7923, grad_fn=<SelectBackward>)\n",
      "b = tensor(32.9810, grad_fn=<SelectBackward>)\n",
      "x * w + b = tensor([33.8772, 58.0729, 59.8652, 83.1648, 52.6961, 47.3193, 38.3579, 25.8119,\n",
      "        43.7347, 56.2806, 70.6189], grad_fn=<AddBackward0>)\n",
      "loss = tensor(9.7754, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "w_b = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "# Adam version\n",
    "optimizer = optim.Adam([w_b], lr=0.1)\n",
    "# quite-nearly torch version\n",
    "for epoch in range(epoches):\n",
    "    loss = functional.mse_loss(w_b[0] * x + w_b[1], y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('w =', w_b[0])\n",
    "print('b =', w_b[1])\n",
    "print('x * w + b =', x * w_b[0] + w_b[1])\n",
    "print('loss =', functional.mse_loss(x * w_b[0] + w_b[1], y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1866f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c059fdd790>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, (x * w_b[0] + w_b[1]).detach().numpy(), '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cca50",
   "metadata": {},
   "source": [
    "## Recipe 3-5. Further Optimizing the Function Problem\n",
    "How do we optimize the training set and test it with a validation set using random samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "114229e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_indices: tensor([ 9, 10,  8,  7,  4,  5,  1,  0,  3])\n",
      "testing_indices: tensor([6, 2])\n"
     ]
    }
   ],
   "source": [
    "n_training = x.shape[0]\n",
    "n_testing = int(0.2 * n_training)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_training)\n",
    "training_indices = shuffled_indices[:-n_testing]\n",
    "testing_indices = shuffled_indices[-n_testing:]\n",
    "\n",
    "print(\"training_indices:\", training_indices)\n",
    "print(\"testing_indices:\", testing_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a548839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = tensor(1.7561, grad_fn=<SelectBackward>)\n",
      "b = tensor(34.0546, grad_fn=<SelectBackward>)\n",
      "x_training_set * w + b = tensor([56.8835, 70.9321, 44.5910, 27.0303, 53.3714, 48.1032, 58.6396, 34.9327,\n",
      "        83.2246], grad_fn=<AddBackward0>)\n",
      "training_loss = tensor(8.8558, grad_fn=<MseLossBackward>)\n",
      "testing_loss = tensor(17.1140, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_training_set = x[training_indices]\n",
    "y_training_set = y[training_indices]\n",
    "x_testing_set = x[testing_indices]\n",
    "y_testing_set = y[testing_indices]\n",
    "\n",
    "w_b = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "optimizer = optim.Adam([w_b], lr=0.1)\n",
    "# can-not-be-more-near torch version\n",
    "# training\n",
    "for epoch in range(epoches):\n",
    "    loss = functional.mse_loss(w_b[0] * x_training_set + w_b[1], y_training_set)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('w =', w_b[0])\n",
    "print('b =', w_b[1])\n",
    "print('x_training_set * w + b =', x_training_set * w_b[0] + w_b[1])\n",
    "print('training_loss =', functional.mse_loss(x_training_set * w_b[0] + w_b[1], y_training_set))\n",
    "print('testing_loss =', functional.mse_loss(x_testing_set * w_b[0] + w_b[1], y_testing_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
