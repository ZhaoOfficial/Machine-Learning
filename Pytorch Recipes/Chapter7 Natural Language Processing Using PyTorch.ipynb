{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b4bf44",
   "metadata": {},
   "source": [
    "# CHAPTER 7\n",
    "**Natural Language Processing Using PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0ed5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2388297f270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae99e54",
   "metadata": {},
   "source": [
    "## Recipe 7-1. Word Embedding\n",
    "Word embedding is the process of representing the words, phrases, and tokens in a meaningful way in a vector structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5680756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"data\": 0, \"science\": 1}\n",
    "embeds = nn.Embedding(2, 5)\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"data\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c1a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a90ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"The popularity of the term \"data science\" has exploded in\n",
    "business environments and academia, as indicated by a jump in job openings.[32]\n",
    "However, many critical academics and journalists see no distinction between data\n",
    "science and statistics. Writing in Forbes, Gil Press argues that data science is a\n",
    "buzzword without a clear definition and has simply replaced \"business analytics\" in\n",
    "contexts such as graduate degree programs.[7] In the question-and-answer section of\n",
    "his keynote address at the Joint Statistical Meetings of American Statistical\n",
    "Association，noted applied statistician Nate Silver said, \"I think data-scientist\n",
    "is a sexed up term for a statistician... Statistics is a branch of science.\n",
    "Data scientist is slightly redundant in some way and people shouldn't berate the\n",
    "term statistician.\"[9] Similarly,in business sector, multiple researchers and\n",
    "analysts state that data scientists alone are far from being sufficient in granting\n",
    "companies a real competitive advantage[33] and consider data scientists as only\n",
    "one of the four greater job families companies require to leverage big\n",
    "data effectively, namely: data analysts, data scientists, big data developers\n",
    "and big data engineers.[34]\n",
    "on the other hand, responses to criticism are as numerous.In a 2014 wall Street\n",
    "Journal article, Irving Wladawsky-Berger compares the data science enthusiasm with\n",
    "the dawn of computer science.He argues data science, like any other interdisciplinary\n",
    "field, employs methodologies and practices from across the academia and industry, but\n",
    "then it will morph them into a new discipline. He brings to attention the sharp criticisms\n",
    "computer science, now a well respected academic discipline, had to once face.[35] Likewise,\n",
    "NYU Stern's Vasant Dhar, as do many other academic proponents of data science, [35] argues\n",
    "more specifically in December 2013 that data science is different from the existing practice\n",
    "of data analysis across all disciplines, which focuses only on explaining data sets.\n",
    "Data science seeks actionable and consistent pattern for predictive uses.[1] This practical\n",
    "engineering goal takes data science beyond traditional analytics. Now the data in those\n",
    "disciplines and applied fields that lacked solid theories,like health science and social\n",
    "science, could be sought and utilized to generate powerful predictive models.[1]\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526f7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf24a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguage(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguage, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(context_size * embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, vocab_size),\n",
    "            nn.LogSoftmax(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        embeds = self.embeddings(x).view((1, -1))\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "losses = []\n",
    "loss_fn = nn.NLLLoss()\n",
    "ngram = NGramLanguage(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = torch.optim.SGD(ngram.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7d2522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1855.1886982917786, 1841.4242477416992, 1827.9246106147766, 1814.6671857833862, 1801.6455292701721, 1788.847277879715, 1776.2672295570374, 1763.9086821079254, 1751.7693364620209, 1739.8531415462494]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "        # step 1. Prepare the inputs to be passed to the model\n",
    "        # (turn the words into integer indices and \n",
    "        # wrap them in tensors)\n",
    "        context_idxs = torch.tensor(\n",
    "            [word_to_ix[w] for w in context],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        ngram.zero_grad()\n",
    "        log_probs = ngram(context_idxs)\n",
    "        loss = loss_fn(\n",
    "            log_probs,\n",
    "            torch.tensor(\n",
    "                [word_to_ix[target]], dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c51592",
   "metadata": {},
   "source": [
    "## Recipe 7-2. CBOW Model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b08560",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"For the future of data science，Donoho projects an ever-growing\n",
    "environment for open science where data sets used for academic publications are\n",
    "accessible to all researchers.[36] US National Institute of Bealth has already announced\n",
    "plans to enhance reproducibility and transparency of research data.[39] other big journals\n",
    "are likewise following suit.[40][41] This way，the future of data science not only exceeds\n",
    "the boundary of statistical theories in scale and methodology，but data science will\n",
    "revolutionize current academia and research paradigms.[36] As Donoho concludes, \"the scope\n",
    "and impact of data science will continue to expand enormously in coming decades as scientific\n",
    "data and data about science itself become ubiquitously available.\"[36]\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cbb308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, vocab_size - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713f4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17d0e8",
   "metadata": {},
   "source": [
    "## Recipe 7-3. LSTM Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
